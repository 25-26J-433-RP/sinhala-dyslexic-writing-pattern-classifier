{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinhala Dyslexia Binary Classifier — Improved Pipeline\n",
    "\n",
    "**Improvements over baseline (v0):**\n",
    "| Area | Baseline | Improved |\n",
    "|---|---|---|\n",
    "| Vectorizer | char TF-IDF (2,4) | char TF-IDF (2,5) + word TF-IDF (1,2) stacked |\n",
    "| Classifier | Logistic Regression | LR + SVM + Voting Ensemble |\n",
    "| Calibration | None | Platt scaling (CalibratedClassifierCV) |\n",
    "| Handcrafted features | None | 8 Sinhala-specific linguistic features |\n",
    "| Essay aggregation | ratio ≥ 0.2 & mean ≥ 0.5 | Weighted by sentence length + peak signal |\n",
    "| Short-text handling | No filtering | Skip sentences < 4 chars |\n",
    "\n",
    "**Baseline accuracy: 78%** → Target: 82–85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:42:37.490746Z",
     "iopub.status.busy": "2026-02-26T08:42:37.490746Z",
     "iopub.status.idle": "2026-02-26T08:42:41.566517Z",
     "shell.execute_reply": "2026-02-26T08:42:41.564955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: pandas in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: joblib in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: scipy in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: numpy in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: filelock in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (4.67.3)\n",
      "Requirement already satisfied: xxhash in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: colorama in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\RP\\sinhala-dyslexic-writing-pattern-classifier\\.venv\\Lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 0. INSTALL DEPENDENCIES\n",
    "# ------------------------------------------------------------\n",
    "# Remove the line below if running locally\n",
    "!pip install datasets pandas scikit-learn joblib scipy numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:42:41.663559Z",
     "iopub.status.busy": "2026-02-26T08:42:41.662554Z",
     "iopub.status.idle": "2026-02-26T08:43:06.564481Z",
     "shell.execute_reply": "2026-02-26T08:43:06.563473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. IMPORTS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:43:06.567479Z",
     "iopub.status.busy": "2026-02-26T08:43:06.566479Z",
     "iopub.status.idle": "2026-02-26T08:43:13.724380Z",
     "shell.execute_reply": "2026-02-26T08:43:13.723336Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 27636 paired sentences\n",
      "['clean_sentence', 'dyslexic_sentence', 'error_type']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_sentence</th>\n",
       "      <th>dyslexic_sentence</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>වලිකුකුළා කෑගහනවා.</td>\n",
       "      <td>වලිකුකුළා කෑගහනව</td>\n",
       "      <td>Grammar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>අම්මා කෑම දෙනවා</td>\n",
       "      <td>අම්මා කෑම දනවා</td>\n",
       "      <td>Phonetic Confusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"correction\": \"අපි ගමට යනවා\", \"analysis\": [{\"...</td>\n",
       "      <td>අපි යනව ගමට</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      clean_sentence dyslexic_sentence  \\\n",
       "0                                 වලිකුකුළා කෑගහනවා.  වලිකුකුළා කෑගහනව   \n",
       "1                                    අම්මා කෑම දෙනවා    අම්මා කෑම දනවා   \n",
       "2  {\"correction\": \"අපි ගමට යනවා\", \"analysis\": [{\"...       අපි යනව ගමට   \n",
       "\n",
       "           error_type  \n",
       "0             Grammar  \n",
       "1  Phonetic Confusion  \n",
       "2             unknown  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. LOAD DATASET\n",
    "# ------------------------------------------------------------\n",
    "# Dataset: paired clean / dyslexic Sinhala sentences\n",
    "# Source: SPEAK-ASR/sinhala-dyslexia-corrected-id20percent\n",
    "\n",
    "dataset = load_dataset(\"SPEAK-ASR/sinhala-dyslexia-corrected-id20percent\")\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "print(f\"Dataset size: {len(df)} paired sentences\")\n",
    "print(df.columns.tolist())\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:43:13.726522Z",
     "iopub.status.busy": "2026-02-26T08:43:13.726522Z",
     "iopub.status.idle": "2026-02-26T08:43:13.768686Z",
     "shell.execute_reply": "2026-02-26T08:43:13.768686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 54979\n",
      "label\n",
      "1    27500\n",
      "0    27479\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. BUILD BINARY CLASSIFICATION DATASET\n",
    "# ------------------------------------------------------------\n",
    "# Label: 1 = dyslexic, 0 = clean\n",
    "\n",
    "dys_df   = pd.DataFrame({\"text\": df[\"dyslexic_sentence\"], \"label\": 1})\n",
    "clean_df = pd.DataFrame({\"text\": df[\"clean_sentence\"],    \"label\": 0})\n",
    "\n",
    "binary_df = pd.concat([dys_df, clean_df], ignore_index=True)\n",
    "\n",
    "# Drop very short sentences (< 4 chars) — unreliable for classification\n",
    "binary_df = binary_df[binary_df[\"text\"].str.len() >= 4].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total samples: {len(binary_df)}\")\n",
    "print(binary_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:43:13.772134Z",
     "iopub.status.busy": "2026-02-26T08:43:13.772134Z",
     "iopub.status.idle": "2026-02-26T08:43:13.818247Z",
     "shell.execute_reply": "2026-02-26T08:43:13.817239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 43983 | Test: 10996\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4. TRAIN / TEST SPLIT\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    binary_df[\"text\"],\n",
    "    binary_df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=binary_df[\"label\"]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train_raw)} | Test: {len(X_test_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:43:13.820934Z",
     "iopub.status.busy": "2026-02-26T08:43:13.820934Z",
     "iopub.status.idle": "2026-02-26T08:43:17.405459Z",
     "shell.execute_reply": "2026-02-26T08:43:17.404439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char features: 41798\n",
      "Word features: 9599\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "#\n",
    "# IMPROVEMENT 1: Dual TF-IDF\n",
    "#   - char_wb (2,5): captures character-level misspellings,\n",
    "#     missing diacritics, and transpositions at word boundaries\n",
    "#   - word (1,2): captures dyslexic word-level patterns\n",
    "#     (wrong word forms, dropped suffixes)\n",
    "#\n",
    "# IMPROVEMENT 2: Sinhala-specific handcrafted features\n",
    "#   - hal_ratio: fraction of hal kirima (්) characters\n",
    "#     Dyslexic writers frequently drop geminate markers\n",
    "#   - diacritic_ratio: vowel diacritics relative to consonants\n",
    "#   - avg_word_len: dyslexic writing tends to have shorter words\n",
    "#   - word_count: sentence length signal\n",
    "#   - unique_char_ratio: variety of Sinhala Unicode codepoints used\n",
    "#   - space_ratio: spacing anomalies in dyslexic text\n",
    "#   - has_english_chars: mixed-script writing pattern\n",
    "#   - repeat_char_ratio: repeated character sequences (perseveration)\n",
    "# ============================================================\n",
    "\n",
    "# ---- TF-IDF Vectorizers ----\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(2, 5),    # extended from (2,4) — captures longer error spans\n",
    "    max_features=60000,\n",
    "    sublinear_tf=True,     # log-scale TF — reduces dominance of frequent n-grams\n",
    "    min_df=2               # ignore extremely rare n-grams\n",
    ")\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=20000,\n",
    "    sublinear_tf=True,\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "# Fit both vectorizers on training data\n",
    "X_train_char = char_vectorizer.fit_transform(X_train_raw)\n",
    "X_test_char  = char_vectorizer.transform(X_test_raw)\n",
    "\n",
    "X_train_word = word_vectorizer.fit_transform(X_train_raw)\n",
    "X_test_word  = word_vectorizer.transform(X_test_raw)\n",
    "\n",
    "print(f\"Char features: {X_train_char.shape[1]}\")\n",
    "print(f\"Word features: {X_train_word.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:43:17.407983Z",
     "iopub.status.busy": "2026-02-26T08:43:17.406476Z",
     "iopub.status.idle": "2026-02-26T08:43:18.156462Z",
     "shell.execute_reply": "2026-02-26T08:43:18.156074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handcrafted features: 8\n"
     ]
    }
   ],
   "source": [
    "# ---- Sinhala-specific handcrafted features ----\n",
    "\n",
    "def extract_sinhala_features(sentences):\n",
    "    \"\"\"\n",
    "    Extract 8 linguistically-motivated features for Sinhala dyslexia detection.\n",
    "    These are computed from Unicode character properties of Sinhala script.\n",
    "\n",
    "    Sinhala Unicode block: U+0D80–U+0DFF\n",
    "      - Consonants: U+0D9A–U+0DC6\n",
    "      - Independent vowels: U+0D85–U+0D96\n",
    "      - Dependent vowel signs: U+0DCF–U+0DDF\n",
    "      - Hal kirima (virama / geminate marker): U+0DCA (්)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for text in sentences:\n",
    "        chars     = list(text)\n",
    "        n         = max(len(chars), 1)\n",
    "        words     = text.split()\n",
    "        nw        = max(len(words), 1)\n",
    "\n",
    "        # 1. hal_ratio — fraction of hal kirima (්) chars\n",
    "        #    Dyslexic writers drop geminate markers: අම්මා → මමා\n",
    "        hal_count  = text.count('\\u0DCA')  # ් character\n",
    "        hal_ratio  = hal_count / n\n",
    "\n",
    "        # 2. diacritic_ratio — vowel diacritics / total chars\n",
    "        #    Missing/wrong diacritics are a core dyslexia indicator\n",
    "        diacritic_count = sum(1 for c in chars if '\\u0DCF' <= c <= '\\u0DDF')\n",
    "        diacritic_ratio = diacritic_count / n\n",
    "\n",
    "        # 3. avg_word_len — average word length in characters\n",
    "        avg_word_len = sum(len(w) for w in words) / nw\n",
    "\n",
    "        # 4. word_count — total word count\n",
    "        word_count = nw\n",
    "\n",
    "        # 5. unique_char_ratio — distinct Sinhala chars / sentence length\n",
    "        #    Dyslexic writers often substitute similar-looking characters\n",
    "        sinhala_chars  = [c for c in chars if '\\u0D80' <= c <= '\\u0DFF']\n",
    "        unique_sinhala = len(set(sinhala_chars))\n",
    "        unique_ratio   = unique_sinhala / max(len(sinhala_chars), 1)\n",
    "\n",
    "        # 6. space_ratio — spaces / total chars\n",
    "        #    Some dyslexic patterns include improper word boundaries\n",
    "        space_ratio = text.count(' ') / n\n",
    "\n",
    "        # 7. has_english — presence of Latin characters (0 or 1)\n",
    "        #    Code-switching is sometimes a dyslexia avoidance strategy\n",
    "        has_english = int(bool(re.search(r'[a-zA-Z]', text)))\n",
    "\n",
    "        # 8. repeat_char_ratio — consecutive repeated characters\n",
    "        #    Perseveration (e.g., азаза) can indicate dyslexic writing\n",
    "        repeats = sum(1 for i in range(1, len(chars)) if chars[i] == chars[i-1])\n",
    "        repeat_ratio = repeats / n\n",
    "\n",
    "        features.append([\n",
    "            hal_ratio,\n",
    "            diacritic_ratio,\n",
    "            avg_word_len,\n",
    "            word_count,\n",
    "            unique_ratio,\n",
    "            space_ratio,\n",
    "            has_english,\n",
    "            repeat_ratio\n",
    "        ])\n",
    "\n",
    "    return csr_matrix(np.array(features, dtype=np.float32))\n",
    "\n",
    "\n",
    "X_train_hf = extract_sinhala_features(X_train_raw.tolist())\n",
    "X_test_hf  = extract_sinhala_features(X_test_raw.tolist())\n",
    "\n",
    "print(f\"Handcrafted features: {X_train_hf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:43:18.158890Z",
     "iopub.status.busy": "2026-02-26T08:43:18.158890Z",
     "iopub.status.idle": "2026-02-26T08:43:18.201692Z",
     "shell.execute_reply": "2026-02-26T08:43:18.200633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined feature dimension: 51405\n"
     ]
    }
   ],
   "source": [
    "# ---- Stack all feature matrices ----\n",
    "#\n",
    "# Final feature vector per sentence:\n",
    "#   [char TF-IDF (60k)] + [word TF-IDF (20k)] + [handcrafted (8)]\n",
    "\n",
    "X_train = hstack([X_train_char, X_train_word, X_train_hf])\n",
    "X_test  = hstack([X_test_char,  X_test_word,  X_test_hf])\n",
    "\n",
    "print(f\"Combined feature dimension: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:43:18.203693Z",
     "iopub.status.busy": "2026-02-26T08:43:18.203693Z",
     "iopub.status.idle": "2026-02-26T08:50:12.648667Z",
     "shell.execute_reply": "2026-02-26T08:50:12.647516Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR calibrated — done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1258: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1258: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1258: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1258: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC calibrated — done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1258: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. MODEL TRAINING — CALIBRATED ENSEMBLE\n",
    "# ============================================================\n",
    "#\n",
    "# IMPROVEMENT 3: Calibrated Logistic Regression\n",
    "#   - CalibratedClassifierCV with Platt scaling corrects the\n",
    "#     probability outputs so 0.65 truly means 65% confidence.\n",
    "#   - Without calibration, raw LR/SVM probabilities can be\n",
    "#     systematically overconfident or underconfident.\n",
    "#\n",
    "# IMPROVEMENT 4: LinearSVC as second learner\n",
    "#   - LinearSVC is often stronger than LR on high-dimensional\n",
    "#     sparse TF-IDF feature spaces.\n",
    "#   - Wrapped in CalibratedClassifierCV to produce probabilities.\n",
    "#\n",
    "# IMPROVEMENT 5: Soft Voting Ensemble\n",
    "#   - Averages probability estimates from both calibrated models.\n",
    "#   - Reduces variance and improves reliability on borderline cases.\n",
    "# ============================================================\n",
    "\n",
    "# Calibrated Logistic Regression\n",
    "lr_base = LogisticRegression(max_iter=1000, C=1.0, solver='saga', n_jobs=-1)\n",
    "lr_calibrated = CalibratedClassifierCV(lr_base, cv=5, method='sigmoid')\n",
    "lr_calibrated.fit(X_train, y_train)\n",
    "print(\"LR calibrated — done\")\n",
    "\n",
    "# Calibrated LinearSVC\n",
    "svc_base = LinearSVC(max_iter=2000, C=0.5)\n",
    "svc_calibrated = CalibratedClassifierCV(svc_base, cv=5, method='sigmoid')\n",
    "svc_calibrated.fit(X_train, y_train)\n",
    "print(\"SVC calibrated — done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:50:12.652829Z",
     "iopub.status.busy": "2026-02-26T08:50:12.651777Z",
     "iopub.status.idle": "2026-02-26T08:50:12.812656Z",
     "shell.execute_reply": "2026-02-26T08:50:12.811646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  Logistic Regression\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80      5496\n",
      "           1       0.82      0.73      0.77      5500\n",
      "\n",
      "    accuracy                           0.79     10996\n",
      "   macro avg       0.79      0.79      0.78     10996\n",
      "weighted avg       0.79      0.79      0.78     10996\n",
      "\n",
      "ROC-AUC: 0.8448\n",
      "Confusion Matrix:\n",
      "[[4613  883]\n",
      " [1480 4020]]\n",
      "\n",
      "==================================================\n",
      "  LinearSVC\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.79      5496\n",
      "           1       0.83      0.71      0.76      5500\n",
      "\n",
      "    accuracy                           0.78     10996\n",
      "   macro avg       0.79      0.78      0.78     10996\n",
      "weighted avg       0.79      0.78      0.78     10996\n",
      "\n",
      "ROC-AUC: 0.8277\n",
      "Confusion Matrix:\n",
      "[[4679  817]\n",
      " [1607 3893]]\n"
     ]
    }
   ],
   "source": [
    "# ---- Individual model evaluation ----\n",
    "\n",
    "for name, mdl in [(\"Logistic Regression\", lr_calibrated), (\"LinearSVC\", svc_calibrated)]:\n",
    "    y_pred = mdl.predict(X_test)\n",
    "    y_prob = mdl.predict_proba(X_test)[:, 1]\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:50:12.815655Z",
     "iopub.status.busy": "2026-02-26T08:50:12.815655Z",
     "iopub.status.idle": "2026-02-26T08:50:12.908904Z",
     "shell.execute_reply": "2026-02-26T08:50:12.907895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  SOFT VOTING ENSEMBLE (LR + SVC)\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.85      0.80      5496\n",
      "           1       0.83      0.72      0.77      5500\n",
      "\n",
      "    accuracy                           0.79     10996\n",
      "   macro avg       0.79      0.79      0.78     10996\n",
      "weighted avg       0.79      0.79      0.78     10996\n",
      "\n",
      "ROC-AUC: 0.8401\n",
      "Confusion Matrix:\n",
      "[[4677  819]\n",
      " [1537 3963]]\n"
     ]
    }
   ],
   "source": [
    "# ---- Soft Voting Ensemble ----\n",
    "#\n",
    "# Averages calibrated probabilities from LR and SVC.\n",
    "# This is done manually (not sklearn VotingClassifier) because\n",
    "# VotingClassifier doesn't natively accept pre-fitted models.\n",
    "\n",
    "lr_probs  = lr_calibrated.predict_proba(X_test)[:, 1]\n",
    "svc_probs = svc_calibrated.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Equal-weight averaging\n",
    "ensemble_probs = (lr_probs + svc_probs) / 2.0\n",
    "ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  SOFT VOTING ENSEMBLE (LR + SVC)\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, ensemble_preds))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, ensemble_probs):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, ensemble_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:50:12.913164Z",
     "iopub.status.busy": "2026-02-26T08:50:12.913164Z",
     "iopub.status.idle": "2026-02-26T08:50:15.050750Z",
     "shell.execute_reply": "2026-02-26T08:50:15.049384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models and vectorizers saved!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. SAVE ALL ARTIFACTS\n",
    "# ============================================================\n",
    "#\n",
    "# Saved artifacts:\n",
    "#   - dyslexia_binary_model_lr.pkl      : calibrated LR model\n",
    "#   - dyslexia_binary_model_svc.pkl     : calibrated SVC model\n",
    "#   - tfidf_char_vectorizer.pkl         : char TF-IDF\n",
    "#   - tfidf_word_vectorizer.pkl         : word TF-IDF\n",
    "#\n",
    "# Legacy compatibility:\n",
    "#   - dyslexia_binary_model.pkl         : best single model (LR)\n",
    "#   - tfidf_vectorizer.pkl              : char vectorizer (for existing code)\n",
    "\n",
    "joblib.dump(lr_calibrated,    \"dyslexia_binary_model_lr.pkl\")\n",
    "joblib.dump(svc_calibrated,   \"dyslexia_binary_model_svc.pkl\")\n",
    "joblib.dump(char_vectorizer,  \"tfidf_char_vectorizer.pkl\")\n",
    "joblib.dump(word_vectorizer,  \"tfidf_word_vectorizer.pkl\")\n",
    "\n",
    "# Legacy aliases (drop-in replacement for existing service code)\n",
    "joblib.dump(lr_calibrated,    \"dyslexia_binary_model.pkl\")\n",
    "joblib.dump(char_vectorizer,  \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"All models and vectorizers saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:50:15.052852Z",
     "iopub.status.busy": "2026-02-26T08:50:15.052852Z",
     "iopub.status.idle": "2026-02-26T08:50:15.509331Z",
     "shell.execute_reply": "2026-02-26T08:50:15.508323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. IMPROVED INFERENCE FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "# Reload all artifacts (simulates production service startup)\n",
    "lr_model       = joblib.load(\"dyslexia_binary_model_lr.pkl\")\n",
    "svc_model      = joblib.load(\"dyslexia_binary_model_svc.pkl\")\n",
    "char_vec       = joblib.load(\"tfidf_char_vectorizer.pkl\")\n",
    "word_vec       = joblib.load(\"tfidf_word_vectorizer.pkl\")\n",
    "\n",
    "\n",
    "def vectorize_sentence_full(sentence: str):\n",
    "    \"\"\"\n",
    "    Converts a Sinhala sentence into the full combined feature vector:\n",
    "      char TF-IDF + word TF-IDF + 8 handcrafted features\n",
    "    \"\"\"\n",
    "    cv = char_vec.transform([sentence])\n",
    "    wv = word_vec.transform([sentence])\n",
    "    hf = extract_sinhala_features([sentence])\n",
    "    return hstack([cv, wv, hf])\n",
    "\n",
    "\n",
    "def predict_sentence_ensemble(sentence: str) -> float:\n",
    "    \"\"\"\n",
    "    Predicts dyslexia probability for a single sentence\n",
    "    using the soft-voting ensemble of LR + SVC.\n",
    "\n",
    "    Returns:\n",
    "        float: Probability of dyslexia (0.0 – 1.0), calibrated\n",
    "    \"\"\"\n",
    "    vec = vectorize_sentence_full(sentence)\n",
    "    lr_p  = lr_model.predict_proba(vec)[0][1]\n",
    "    svc_p = svc_model.predict_proba(vec)[0][1]\n",
    "    return float((lr_p + svc_p) / 2.0)\n",
    "\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Splits essay text into sentences.\n",
    "    Handles: Sinhala punctuation, danda (।), newlines.\n",
    "    Filters out fragments shorter than 4 characters.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    raw  = re.split(r\"[.!?।\\n]+\", text)\n",
    "    cleaned = [s.strip() for s in raw if len(s.strip()) >= 4]\n",
    "\n",
    "    # Chunk long single-paragraph essays\n",
    "    if len(cleaned) == 1 and len(cleaned[0]) > 200:\n",
    "        long_text = cleaned[0]\n",
    "        cleaned = [long_text[i:i+120] for i in range(0, len(long_text), 120)]\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def analyze_essay(essay_text: str, threshold: float = 0.65) -> dict:\n",
    "    \"\"\"\n",
    "    Performs essay-level dyslexia analysis.\n",
    "\n",
    "    IMPROVEMENT 6: Weighted essay aggregation\n",
    "      - Longer sentences carry more weight (more signal per sentence)\n",
    "      - Peak probability is included as a strong indicator\n",
    "      - Composite score: 60% weighted_mean + 40% peak_probability\n",
    "      - This prevents short trivial sentences from diluting the score\n",
    "\n",
    "    IMPROVEMENT 7: Three-tier sentence labeling\n",
    "      - NORMAL    : prob < 0.50\n",
    "      - BORDERLINE: 0.50 <= prob < threshold\n",
    "      - DYSLEXIC  : prob >= threshold\n",
    "    \"\"\"\n",
    "    sentences = split_sentences(essay_text)\n",
    "\n",
    "    if not sentences:\n",
    "        return {\"error\": \"No valid sentences found.\"}\n",
    "\n",
    "    probabilities    = []\n",
    "    sentence_results = []\n",
    "    dyslexic_count   = 0\n",
    "    borderline_count = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        prob = predict_sentence_ensemble(s)\n",
    "        probabilities.append(prob)\n",
    "\n",
    "        # Three-tier labeling\n",
    "        if prob >= threshold:\n",
    "            label = \"DYSLEXIC\"\n",
    "            dyslexic_count += 1\n",
    "        elif prob >= 0.50:\n",
    "            label = \"BORDERLINE\"\n",
    "            borderline_count += 1\n",
    "        else:\n",
    "            label = \"NORMAL\"\n",
    "\n",
    "        sentence_results.append({\n",
    "            \"text\": s,\n",
    "            \"probability\": round(float(prob), 3),\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "    # ---- Weighted essay aggregation ----\n",
    "    # Weight each sentence by its word count (longer = more signal)\n",
    "    weights       = [max(len(s.split()), 1) for s in sentences]\n",
    "    total_weight  = sum(weights)\n",
    "    weighted_mean = sum(p * w for p, w in zip(probabilities, weights)) / total_weight\n",
    "    peak_prob     = max(probabilities)\n",
    "\n",
    "    # Composite score: weighted mean (60%) + peak signal (40%)\n",
    "    composite_score = 0.6 * weighted_mean + 0.4 * peak_prob\n",
    "\n",
    "    # Essay-level decision\n",
    "    dyslexic_ratio = dyslexic_count / len(sentences)\n",
    "\n",
    "    if composite_score >= 0.55 or (dyslexic_ratio >= 0.2 and weighted_mean >= 0.5):\n",
    "        essay_label = \"DYSLEXIC ESSAY\"\n",
    "    elif composite_score >= 0.45:\n",
    "        essay_label = \"BORDERLINE ESSAY\"\n",
    "    else:\n",
    "        essay_label = \"NORMAL ESSAY\"\n",
    "\n",
    "    return {\n",
    "        \"essay_label\":              essay_label,\n",
    "        \"composite_score\":          round(composite_score, 3),\n",
    "        \"weighted_mean_prob\":       round(weighted_mean, 3),\n",
    "        \"peak_sentence_prob\":       round(peak_prob, 3),\n",
    "        \"dyslexic_ratio\":           round(dyslexic_ratio, 3),\n",
    "        \"total_sentences\":          len(sentences),\n",
    "        \"dyslexic_sentences\":       dyslexic_count,\n",
    "        \"borderline_sentences\":     borderline_count,\n",
    "        \"sentences\":                sentence_results\n",
    "    }\n",
    "\n",
    "print(\"Inference functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:50:15.512336Z",
     "iopub.status.busy": "2026-02-26T08:50:15.512336Z",
     "iopub.status.idle": "2026-02-26T08:50:15.587873Z",
     "shell.execute_reply": "2026-02-26T08:50:15.586817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NORMAL ESSAY ---\n",
      "Label: BORDERLINE ESSAY\n",
      "Composite score: 0.531\n",
      "  [BORDERLINE] 0.642  මම අද පාසලට ගියෙමි\n",
      "  [NORMAL    ] 0.401  ගුරුතුමා අපට ගණිත පාඩම ඉගැන්වීය\n",
      "  [NORMAL    ] 0.380  විවේක කාලයේදී මිතුරන් සමඟ කතා කළෙමි\n",
      "\n",
      "--- DYSLEXIC ESSAY ---\n",
      "Label: DYSLEXIC ESSAY\n",
      "Composite score: 0.845\n",
      "  [DYSLEXIC  ] 0.762  මම අද පාසල් ගිය\n",
      "  [BORDERLINE] 0.609  ගුරුතුමා අපට ගනිත පාඩම ඉගැන්වය\n",
      "  [DYSLEXIC  ] 0.940  විවේක කලයෙදි මිතුරන් සමග කතාකර ගිය\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. MANUAL TEST CASES\n",
    "# ============================================================\n",
    "\n",
    "normal_essay = \"\"\"\n",
    "මම අද පාසලට ගියෙමි. ගුරුතුමා අපට ගණිත පාඩම ඉගැන්වීය.\n",
    "විවේක කාලයේදී මිතුරන් සමඟ කතා කළෙමි.\n",
    "\"\"\"\n",
    "\n",
    "dyslexic_essay = \"\"\"\n",
    "මම අද පාසල් ගිය. ගුරුතුමා අපට ගනිත පාඩම ඉගැන්වය.\n",
    "විවේක කලයෙදි මිතුරන් සමග කතාකර ගිය.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- NORMAL ESSAY ---\")\n",
    "result = analyze_essay(normal_essay)\n",
    "print(f\"Label: {result['essay_label']}\")\n",
    "print(f\"Composite score: {result['composite_score']}\")\n",
    "for s in result['sentences']:\n",
    "    print(f\"  [{s['label']:10}] {s['probability']:.3f}  {s['text']}\")\n",
    "\n",
    "print(\"\\n--- DYSLEXIC ESSAY ---\")\n",
    "result = analyze_essay(dyslexic_essay)\n",
    "print(f\"Label: {result['essay_label']}\")\n",
    "print(f\"Composite score: {result['composite_score']}\")\n",
    "for s in result['sentences']:\n",
    "    print(f\"  [{s['label']:10}] {s['probability']:.3f}  {s['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T08:50:15.591872Z",
     "iopub.status.busy": "2026-02-26T08:50:15.590876Z",
     "iopub.status.idle": "2026-02-26T08:52:16.227683Z",
     "shell.execute_reply": "2026-02-26T08:52:16.226184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synthetic essay-level accuracy: 0.5894 (1296/2199)\n",
      "(Note: essays grouped from test sentences, not real essays)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. ESSAY-LEVEL CROSS-VALIDATION SIMULATION\n",
    "# ============================================================\n",
    "#\n",
    "# Since the model is trained at sentence level, we simulate\n",
    "# essay-level performance by grouping test sentences into\n",
    "# synthetic essays of 5 sentences each and checking whether\n",
    "# the essay label matches the majority class of its sentences.\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"text\":  X_test_raw.tolist(),\n",
    "    \"label\": y_test.tolist()\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Group into synthetic essays (5 sentences each)\n",
    "ESSAY_SIZE = 5\n",
    "essay_correct = 0\n",
    "essay_total   = 0\n",
    "\n",
    "for i in range(0, len(test_df) - ESSAY_SIZE, ESSAY_SIZE):\n",
    "    chunk      = test_df.iloc[i:i+ESSAY_SIZE]\n",
    "    essay_text = \". \".join(chunk[\"text\"].tolist())\n",
    "    true_label = 1 if chunk[\"label\"].mean() >= 0.5 else 0\n",
    "\n",
    "    result = analyze_essay(essay_text)\n",
    "    pred_label = 1 if \"DYSLEXIC\" in result[\"essay_label\"] else 0\n",
    "\n",
    "    if pred_label == true_label:\n",
    "        essay_correct += 1\n",
    "    essay_total += 1\n",
    "\n",
    "essay_accuracy = essay_correct / essay_total\n",
    "print(f\"\\nSynthetic essay-level accuracy: {essay_accuracy:.4f} ({essay_correct}/{essay_total})\")\n",
    "print(\"(Note: essays grouped from test sentences, not real essays)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Improvements\n",
    "\n",
    "### Changes vs. Baseline (v0)\n",
    "\n",
    "**Feature Engineering**\n",
    "- Extended char n-gram range from `(2,4)` → `(2,5)` to capture longer misspelling spans\n",
    "- Added `sublinear_tf=True` to reduce dominance of high-frequency n-grams\n",
    "- Added word-level TF-IDF `(1,2)` stacked with char TF-IDF\n",
    "- Added 8 Sinhala-specific handcrafted features (hal ratio, diacritic ratio, avg word length, etc.)\n",
    "\n",
    "**Model**\n",
    "- Baseline used plain `LogisticRegression` — probabilities uncalibrated\n",
    "- Improved: `CalibratedClassifierCV` with 5-fold Platt scaling → reliable probabilities\n",
    "- Added `LinearSVC` as a second learner\n",
    "- Soft voting ensemble (average of LR + SVC calibrated probabilities)\n",
    "\n",
    "**Essay Aggregation**\n",
    "- Baseline: flat mean + ratio threshold\n",
    "- Improved: word-count weighted mean + peak signal composite\n",
    "- Three-tier labeling: NORMAL / BORDERLINE / DYSLEXIC\n",
    "- New output field: `composite_score` for transparent ranking\n",
    "\n",
    "**Data Cleaning**\n",
    "- Drop sentences shorter than 4 characters (unreliable signal)\n",
    "\n",
    "### Updated Inference Files\n",
    "Update `vectorizer.py` to use both vectorizers + handcrafted features.\n",
    "Update `sentence_classifier.py` to load both models and average predictions.\n",
    "The `dyslexia_binary_model.pkl` and `tfidf_vectorizer.pkl` legacy aliases are saved\n",
    "for backward compatibility with the existing API service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
